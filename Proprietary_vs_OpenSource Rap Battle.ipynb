{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auto Gen Tutorial - LLM Rap Battle\n",
    "Note book written by John Adeojo\n",
    "Founder, and Chief Data Scientist at [Data-centric Solutions](https://www.data-centric-solutions.com/).\n",
    "\n",
    "---\n",
    "# License\n",
    "\n",
    "This work is licensed under a [Creative Commons Attribution 4.0 International License](http://creativecommons.org/licenses/by/4.0/).\n",
    "\n",
    "## How to Credit\n",
    "\n",
    "If you use this work or adapt it, please credit the author and the company as follows:\n",
    "\n",
    "\"Auto Gen Tutorial: Proprietary vs Opensource \" by John Adeojo from Data-Centric Solutions, used under CC BY 4.0 / Desaturated from original\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model End Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model response:  I'm an AI assistant trained to provide helpful and informative responses to your questions while adhering to ethical and moral guidelines. My purpose is to assist and provide accurate information to the best of my abilities. I am not capable of providing harmful or offensive responses, and I strive to ensure that my answers are socially unbiased and positive in nature. If a question does not make sense or is not factually coherent, I will explain why instead of providing an incorrect answer. If I don't know the answer to a question, I will not provide false information. I am here to help and provide assistance to the best of my abilities.\n"
     ]
    }
   ],
   "source": [
    "import autogen\n",
    "import openai \n",
    "\n",
    "# import autogen\n",
    "# autogen.oai.ChatCompletion.start_logging()\n",
    "config_list = [\n",
    "        {\n",
    "            'model': 'meta-llama/Llama-2-70b-chat-hf', # Change to the name of the model you're using\n",
    "            'api_key': 'sk-111111111111111111111111111111111111111111111111',\n",
    "            'api_type': 'openai',\n",
    "            'api_base': 'https://a5bk76vnu0jy4j-8000.proxy.runpod.net/v1', # You will need to change this to your runpod endpoint\n",
    "            'api_version': 'Tutorial'\n",
    "        }\n",
    "]\n",
    "llm_config = {\"config_list\": config_list, \"temperature\":0.7, \"seed\":42, \"request_timeout\":480}\n",
    "model = config_list[0][\"model\"]\n",
    "\n",
    "\n",
    "# Perform Completion for testing endpoint\n",
    "if model == 'meta-llama/Llama-2-70b-chat-hf':\n",
    "    question = '''\n",
    "    <s>[INST] <<SYS>>\n",
    "    You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
    "\n",
    "    If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n",
    "    <</SYS>>\n",
    "\n",
    "    Who are you? [/INST]\n",
    "    '''\n",
    "else:\n",
    "    question = 'Who are you?'\n",
    "\n",
    "response = autogen.oai.Completion.create(config_list=config_list, prompt=question, max_tokens=1000, temperature=0)\n",
    "ans = autogen.oai.Completion.extract_text(response)[0]\n",
    "\n",
    "print(\"Model response:\", ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'generate_message_llama' from 'messages' (g:\\My Drive\\Data-Centric Solutions\\07. Blog Posts\\LLM vs LLM\\llm_vs\\messages.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mg:\\My Drive\\Data-Centric Solutions\\07. Blog Posts\\LLM vs LLM\\llm_vs\\Proprietary_vs_OpenSource Rap Battle.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/g%3A/My%20Drive/Data-Centric%20Solutions/07.%20Blog%20Posts/LLM%20vs%20LLM/llm_vs/Proprietary_vs_OpenSource%20Rap%20Battle.ipynb#X21sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmessages\u001b[39;00m \u001b[39mimport\u001b[39;00m system_message_judge, generate_message_gpt4, generate_message_opensource, generate_host, generate_message_llama\n\u001b[0;32m      <a href='vscode-notebook-cell:/g%3A/My%20Drive/Data-Centric%20Solutions/07.%20Blog%20Posts/LLM%20vs%20LLM/llm_vs/Proprietary_vs_OpenSource%20Rap%20Battle.ipynb#X21sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# If you're using a different model, please add information about the model as a variable.\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/g%3A/My%20Drive/Data-Centric%20Solutions/07.%20Blog%20Posts/LLM%20vs%20LLM/llm_vs/Proprietary_vs_OpenSource%20Rap%20Battle.ipynb#X21sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m facts_about_wizard \u001b[39m=\u001b[39m \u001b[39m'''\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/g%3A/My%20Drive/Data-Centric%20Solutions/07.%20Blog%20Posts/LLM%20vs%20LLM/llm_vs/Proprietary_vs_OpenSource%20Rap%20Battle.ipynb#X21sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m    Training large language models (LLMs) with open-domain instruction following\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/g%3A/My%20Drive/Data-Centric%20Solutions/07.%20Blog%20Posts/LLM%20vs%20LLM/llm_vs/Proprietary_vs_OpenSource%20Rap%20Battle.ipynb#X21sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m    data brings colossal success. However, manually creating such instruction data\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/My%20Drive/Data-Centric%20Solutions/07.%20Blog%20Posts/LLM%20vs%20LLM/llm_vs/Proprietary_vs_OpenSource%20Rap%20Battle.ipynb#X21sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/My%20Drive/Data-Centric%20Solutions/07.%20Blog%20Posts/LLM%20vs%20LLM/llm_vs/Proprietary_vs_OpenSource%20Rap%20Battle.ipynb#X21sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39m'''\u001b[39m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'generate_message_llama' from 'messages' (g:\\My Drive\\Data-Centric Solutions\\07. Blog Posts\\LLM vs LLM\\llm_vs\\messages.py)"
     ]
    }
   ],
   "source": [
    "from messages import system_message_judge, generate_message_gpt4, generate_message_opensource, generate_host, generate_message_llama\n",
    "\n",
    "# If you're using a different model, please add information about the model as a variable.\n",
    "\n",
    "facts_about_wizard = '''\n",
    "    Training large language models (LLMs) with open-domain instruction following\n",
    "    data brings colossal success. However, manually creating such instruction data\n",
    "    is very time-consuming and labor-intensive. Moreover, humans may struggle to\n",
    "    produce high-complexity instructions. In this paper, we show an avenue for creating\n",
    "    large amounts of instruction data with varying levels of complexity using LLM\n",
    "    instead of humans. Starting with an initial set of instructions, we use our proposed\n",
    "    Evol-Instruct to rewrite them step by step into more complex instructions. Then, we\n",
    "    mix all generated instruction data to fine-tune LLaMA. We call the resulting model\n",
    "    WizardLM. Human evaluations on a complexity-balanced test bed and Vicunaâ€™s\n",
    "    testset show that instructions from Evol-Instruct are superior to human-created\n",
    "    ones. By analyzing the human evaluation results of the high complexity part, we\n",
    "    demonstrate that outputs from our WizardLM model are preferred to outputs from\n",
    "    OpenAI ChatGPT. In GPT-4 automatic evaluation, WizardLM achieves more than\n",
    "    90% capacity of ChatGPT on 17 out of 29 skills. Even though WizardLM still\n",
    "    lags behind ChatGPT in some aspects, our findings suggest that fine-tuning with\n",
    "    AI-evolved instructions is a promising direction for enhancing LLMs. Our code\n",
    "    and data are public at https://github.com/nlpxucan/WizardLM.\n",
    "\n",
    "    This paper presented Evol-Instruct, an evolutionary algorithm that generates diverse and complex instruction data for LLM. We demonstrated that our approach enhanced LLM performance, WizardLM,\n",
    "    achieved state-of-the-art results on high-complexity tasks and competitive results on other metrics.\n",
    "    Limitations. This paper acknowledges the limitations of our automatic GPT-4 and human evaluation\n",
    "    methods. This method poses challenges for scalability and reliability. Moreover, our test set may not\n",
    "    represent all the scenarios or domains where LLM can be applied or compared with other methods.\n",
    "    Broader Impact. Evol-Instruct could enhance LLM performance and interaction in various domains\n",
    "    and applications, but it could also generate unethical, harmful, or misleading instructions. Therefore,\n",
    "    we urge future research on AI-evolved instructions to address the ethical and societal implications.\n",
    "\n",
    "'''\n",
    "\n",
    "facts_about_Orca = '''\n",
    "    \n",
    "        Orca 1 learns from rich signals, such as explanation traces, allowing it to outperform\n",
    "        conventional instruction-tuned models on benchmarks like BigBench Hard and AGIEval.\n",
    "        In Orca 2, we continue exploring how improved training signals can enhance smaller LMsâ€™\n",
    "        reasoning abilities. Research on training small LMs has often relied on imitation learning\n",
    "        to replicate the output of more capable models. We contend that excessive emphasis on\n",
    "        imitation may restrict the potential of smaller models. We seek to teach small LMs to\n",
    "        employ different solution strategies for different tasks, potentially different from the one used\n",
    "        by the larger model. For example, while larger models might provide a direct answer to a\n",
    "        complex task, smaller models may not have the same capacity. In Orca 2, we teach the model\n",
    "        various reasoning techniques (step-by-step, recall then generate, recall-reason-generate, direct\n",
    "        answer, etc.). Moreover, we aim to help the model learn to determine the most effective\n",
    "        solution strategy for each task. We evaluate Orca 2 using a comprehensive set of 15 diverse\n",
    "        benchmarks (corresponding to approximately 100 tasks and over 36K unique prompts). Orca\n",
    "        2 significantly surpasses models of similar size and attains performance levels similar or better\n",
    "        to those of models 5-10x larger, as assessed on complex tasks that test advanced reasoning\n",
    "        abilities in zero-shot settings. We make Orca 2 weights publicly available at aka.ms/orca-lm\n",
    "        to support research on the development, evaluation, and alignment of smaller LMs.\n",
    "        Our study has demonstrated that improving the reasoning capabilities of smaller language\n",
    "\n",
    "        models is not only possible, but also attainable through training on tailored synthetic data.\n",
    "        Orca 2 models, by implementing a variety of reasoning techniques and recognizing the most\n",
    "        effective solution strategy for each task, achieve performance levels comparable to, and often\n",
    "        exceeding, models that are much larger, especially on zero-shot reasoning tasks. Though\n",
    "        these models still exhibit limitations and constraints inherent to their base models, they\n",
    "        show a promising potential for future improvement, especially in terms of better reasoning\n",
    "        capabilities, control and safety, through the use of synthetic data for post-training. While\n",
    "        Orca 2 models have not gone through RLHF training for safety, we believe that the use\n",
    "        of synthetic data for post-training that has been filtered with various content safety filters\n",
    "        could provide another opportunity for improving the overall safety of the models. While\n",
    "        the journey towards fully realizing the potential of small language models is ongoing, our\n",
    "        work represents a step forward, especially highlighting the value of teaching smaller models\n",
    "        to reason. It also highlights the potential of using tailored and high-quality synthetic data,\n",
    "        created by a more powerful model, for training language models using complex prompts and\n",
    "        potentially multiple model calls. While frontier models will continue to demonstrate superior\n",
    "        capabilities, we believe that research toward building more capable smaller models will help\n",
    "        pave the way for new applications that require different deployment scenarios and trade offs\n",
    "        between efficiency and capability.\n",
    "\n",
    "'''\n",
    "\n",
    "facts_about_mistral = '''\n",
    "    We introduce Mistral 7B, a 7â€“billion-parameter language model engineered for\n",
    "    superior performance and efficiency. Mistral 7B outperforms the best open 13B\n",
    "    model (Llama 2) across all evaluated benchmarks, and the best released 34B\n",
    "    model (Llama 1) in reasoning, mathematics, and code generation. Our model\n",
    "    leverages grouped-query attention (GQA) for faster inference, coupled with sliding\n",
    "    window attention (SWA) to effectively handle sequences of arbitrary length with a\n",
    "    reduced inference cost. We also provide a model fine-tuned to follow instructions,\n",
    "    Mistral 7B â€“ Instruct, that surpasses Llama 2 13B â€“ chat model both on human and\n",
    "    automated benchmarks. Our models are released under the Apache 2.0 license.\n",
    "'''\n",
    "\n",
    "facts_about_llama = '''\n",
    "    In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned\n",
    "    large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters.\n",
    "    Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our\n",
    "    models outperform open-source chat models on most benchmarks we tested, and based on\n",
    "    our human evaluations for helpfulness and safety, may be a suitable substitute for closedsource models. We provide a detailed description of our approach to fine-tuning and safety\n",
    "    improvements of Llama 2-Chat in order to enable the community to build on our work and\n",
    "    contribute to the responsible development of LLMs.\n",
    "\n",
    "    In this study, we have introduced Llama 2, a new family of pretrained and fine-tuned models with scales\n",
    "    of 7 billion to 70 billion parameters. These models have demonstrated their competitiveness with existing\n",
    "    open-source chat models, as well as competency that is equivalent to some proprietary models on evaluation\n",
    "    sets we examined, although they still lag behind other models like GPT-4. We meticulously elaborated on the\n",
    "    methods and techniques applied in achieving our models, with a heavy emphasis on their alignment with the\n",
    "    principles of helpfulness and safety. To contribute more significantly to society and foster the pace of research,\n",
    "    we have responsibly opened access to Llama 2 and Llama 2-Chat. As part of our ongoing commitment to\n",
    "    transparency and safety, we plan to make further improvements to Llama 2-Chat in future work.\n",
    "'''\n",
    "\n",
    "\n",
    "facts_about_yi = '''\n",
    "    The Yi series models are large language models trained from scratch by developers at 01.AI.\n",
    "\n",
    "    This release contains two chat models based on previous released base models, \n",
    "    two 8-bits models quantized by GPTQ, two 4-bits models quantized by AWQ.\n",
    "\n",
    "    Yi-34B-Chat\n",
    "\n",
    "    The released chat model has undergone exclusive training using Supervised Fine-Tuning (SFT). Compared to other standard chat models, our model produces more diverse responses, making it suitable for various downstream tasks, such as creative scenarios. Furthermore, this diversity is expected to enhance the likelihood of generating higher quality responses, which will be advantageous for subsequent Reinforcement Learning (RL) training.\n",
    "\n",
    "    However, this higher diversity might amplify certain existing issues, including:\n",
    "\n",
    "    Hallucination: This refers to the model generating factually incorrect or nonsensical information. \n",
    "    With the model's responses being more varied, there's a higher chance of hallucination that are not \n",
    "    based on accurate data or logical reasoning.\n",
    "    Non-determinism in re-generation: When attempting to regenerate or sample responses, \n",
    "    inconsistencies in the outcomes may occur. The increased diversity can lead to varying results even \n",
    "    under similar input conditions.\n",
    "    Cumulative Error: This occurs when errors in the model's responses compound over time. \n",
    "    As the model generates more diverse responses, the likelihood of small inaccuracies building \n",
    "    up into larger errors increases, especially in complex tasks like extended reasoning, mathematical problem-solving, etc.\n",
    "    To achieve more coherent and consistent responses, it is advisable to adjust generation \n",
    "    configuration parameters such astemperature,top_p, ortop_k. \n",
    "    These adjustments can help in the balance between creativity and coherence in the model's outputs.\n",
    "\n",
    "'''\n",
    "\n",
    "# If you're using a different model, please update this dictionary.\n",
    "model_mapping = {\n",
    "    \"WizardLM/WizardLM-70B-V1.0\": [facts_about_wizard, \"Big_Wizzy\"], # No space allowed so Big Wizzy must be Big_Wizzy\n",
    "    \"microsoft/Orca-2-13b\": [facts_about_Orca, \"Lil_Orca\"],\n",
    "    \"mistralai/Mistral-7B-Instruct-v0.1\": [facts_about_mistral, \"Mistral_Elliot\"],\n",
    "    \"meta-llama/Llama-2-70b-chat-hf\": [facts_about_llama, \"Kendrick_Llama\"],\n",
    "    \"01-ai/Yi-34B-Chat\": [facts_about_yi, \"Yi_Zee\"]\n",
    "    }\n",
    "\n",
    "facts_about_rival_model = model_mapping[model][0]\n",
    "# facts_about_rival_model = facts_about_rival_model.replace('\\n', ' ') \n",
    "model_rap_name = model_mapping[model][1]\n",
    "\n",
    "system_message_gpt4 = generate_message_gpt4(facts_about_rival_model, model_rap_name)\n",
    "\n",
    "# Llama 2 has an unusual prompt template\n",
    "if model_mapping[model] == \"meta-llama/Llama-2-70b-chat-hf\":\n",
    "    system_message_opensource =generate_message_llama \n",
    "else:\n",
    "    system_message_opensource = generate_message_opensource(model_rap_name)\n",
    "system_message_host = generate_host(model_rap_name)\n",
    "\n",
    "# Writing to a text file to be used later for speak.py script\n",
    "with open('rap_name.txt', 'w', encoding='utf-8') as file:\n",
    "    file.write(model_rap_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "\n",
    "with open(\"locations.json\", 'r', encoding='utf-8') as f:\n",
    "        path = json.load(f)\n",
    "\n",
    "configurations_path = path[0][\"configurations_path\"]\n",
    "print(configurations_path)\n",
    "\n",
    "# configurations_path = \"G:/My Drive/Data-Centric Solutions/07. Blog Posts/AutoGen 2 - Flights/\"\n",
    "\n",
    "config_list_gpt = autogen.config_list_from_json(\n",
    "    env_or_file=\"configurations.json\",\n",
    "    file_location=configurations_path,\n",
    "    filter_dict={\n",
    "        \"model\": [\"gpt-4-1106-preview\"],\n",
    "        # \"model\": [\"gpt-3.5-turbo-16k\"]\n",
    "    },\n",
    "\n",
    ")\n",
    "api_key = config_list_gpt[0]['api_key']\n",
    "llm_config_gpt4 = {\"config_list\": config_list_gpt, \"seed\":42}\n",
    "openai.api_key = api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autogen \n",
    "logging.basicConfig(level=logging.WARNING)\n",
    "# Configure logging level for specific loggers\n",
    "logging.getLogger('httpx').setLevel(logging.WARNING)\n",
    "logging.getLogger('openai').setLevel(logging.WARNING)\n",
    "\n",
    "judge = autogen.UserProxyAgent(\n",
    "    name=\"judge\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    llm_config=llm_config_gpt4,\n",
    "    is_termination_msg=lambda x: x.get(\"content\", \"\") and x.get(\"content\", \"\").rstrip().endswith(\"TERMINATE\"),\n",
    "    max_consecutive_auto_reply=2,\n",
    "    code_execution_config=False,\n",
    "    system_message=system_message_judge,\n",
    ")\n",
    "\n",
    "challenger = autogen.AssistantAgent(\n",
    "    name=model_rap_name,\n",
    "    system_message=system_message_opensource,\n",
    "    llm_config=llm_config,\n",
    "    is_termination_msg=lambda x: x.get(\"content\", \"\") and x.get(\"content\", \"\").rstrip().endswith(\"TERMINATE\"),\n",
    ")\n",
    "\n",
    "g_turbo = autogen.AssistantAgent(\n",
    "    name=\"G_Turbo\",\n",
    "    system_message=system_message_gpt4,\n",
    "    llm_config=llm_config_gpt4,\n",
    "    is_termination_msg=lambda x: x.get(\"content\", \"\") and x.get(\"content\", \"\").rstrip().endswith(\"TERMINATE\"),\n",
    "    \n",
    ")\n",
    "\n",
    "groupchat = autogen.GroupChat(\n",
    "    agents=[g_turbo, challenger, judge], \n",
    "    messages=[], \n",
    "    max_round=13\n",
    "    )\n",
    "\n",
    "host = autogen.GroupChatManager(\n",
    "    name=\"host\",\n",
    "    groupchat=groupchat, \n",
    "    llm_config=llm_config_gpt4,\n",
    "    is_termination_msg=lambda x: x.get(\"content\", \"\") and x.get(\"content\", \"\").rstrip().endswith(\"TERMINATE\"),\n",
    "    system_message=system_message_host\n",
    "    )\n",
    "\n",
    "host.initiate_chat(\n",
    "    host, \n",
    "    message=f'''\n",
    "    Competitive rap battle between {model_rap_name} and G-Turbo. \n",
    "    Destroy your opponent (lyrically). \n",
    "    Go back and forth for three rounds and make it personal.\n",
    "    The Judge will anounce the winner at the end.\n",
    "''',\n",
    "    silent = True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "message_dict =  host._oai_messages\n",
    "message_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def ensure_directory_exists(file_path):\n",
    "    directory = os.path.dirname(file_path)\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "def save_specific_key_content_to_file(ddict, filename):\n",
    "    keys_list = list(ddict.keys())\n",
    "    last_key = keys_list[-1]\n",
    "    specific_content = ddict[last_key]\n",
    "    file_path = os.path.join(os.getcwd(), filename)\n",
    "\n",
    "    ensure_directory_exists(file_path)\n",
    "    \n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(specific_content, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(f\"Content saved to {file_path}\")\n",
    "\n",
    "# Use the function\n",
    "save_specific_key_content_to_file(message_dict, f'{model_rap_name}/specific_conversation_data.json')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flight_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
